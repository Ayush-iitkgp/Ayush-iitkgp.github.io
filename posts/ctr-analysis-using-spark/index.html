<!DOCTYPE html>
<html prefix="" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Click Through Rate Analysis using Spark | Ayush Pandey</title>
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/poole.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/lanyon.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://Ayush-iitkgp.github.io/posts/ctr-analysis-using-spark/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Ayush Pandey">
<link rel="prev" href="../gsoc-2017/" title="Google Summer of Code 2017" type="text/html">
<link rel="next" href="../building-machine-learning-data-pipeline-using-apache-spark/" title="Building Machine Learning Data Pipeline using Apache Spark" type="text/html">
<meta property="og:site_name" content="Ayush Pandey">
<meta property="og:title" content="Click Through Rate Analysis using Spark">
<meta property="og:url" content="http://Ayush-iitkgp.github.io/posts/ctr-analysis-using-spark/">
<meta property="og:description" content="Introduction
In recent years, programmatic advertising is been taking over the online advertisement industry. To enable automatic selling and purchasing ad impressions between advertisers and publishe">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2019-02-21T02:13:21+05:30">
<meta property="article:tag" content="AdTech">
<meta property="article:tag" content="Data Science">
<meta property="article:tag" content="Machine Learning">
</head>
<body class="theme-base-0d">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>
    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
            styles, `#sidebar-checkbox` for behavior. -->
    <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox"><!-- Toggleable sidebar --><div class="sidebar" id="sidebar">
        <!---<div class="sidebar-item">
            <p>A reserved <a href="https://getnikola.com" target="_blank">Nikola</a> theme that places the utmost gravity on content with a hidden drawer. Made by <a href="https://twitter.com/mdo" target="_blank">@mdo</a> for Jekyll,
            ported to Nikola by <a href="https://twitter.com/ralsina" target="_blank">@ralsina</a>.</p>
        </div> -->
        
    <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../../blog">Blog</a>
        <a class="sidebar-nav-item" href="https://drive.google.com/open?id=1NMfMEWj8fj9yLBgl0IsKBVPjyA9Ik5y4">Resume</a>
        <a class="sidebar-nav-item" href="https://drive.google.com/drive/folders/0B2oOdWdSJWa1RWRsQWdCZ25LUXc">University Projects</a>
        <a class="sidebar-nav-item" href="../gsoc-2017/">GSoC'17</a>
        <a class="sidebar-nav-item" href="../is-it-the-end-or-another-beginning/">GSoC'16</a>
    
    
    </nav>
</div>

    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          
    <h3 id="brand" class="masthead-title">
      <a href="http://Ayush-iitkgp.github.io/" title="Ayush Pandey" rel="home">Ayush Pandey</a>
    </h3>

        </div>
      </div>

      <div class="container content" id="content">
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="post-title p-name entry-title" itemprop="headline name"><a href="." class="u-url">Click Through Rate Analysis using Spark</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">Ayush Pandey</span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="post-date published dt-published" datetime="2019-02-21T02:13:21+05:30" itemprop="datePublished" title="2019-02-21 02:13">2019-02-21 02:13</time></a></p>
        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<h2>Introduction</h2>
<p>In recent years, programmatic advertising is been taking over the online advertisement industry. To enable automatic selling and purchasing ad impressions between advertisers and publishers through real-time auctions, Real-Time Bidding (RTB) is quickly becoming the leading method.</p>
<p>In contrast to the traditional online ad market, where a certain amount of impressions is sold at a fixed rate, RTB allows advertisers to bid each impression individually in real time at a cost based on impression-level features. Real-time Bidding (RTB) is a way of transacting media that allows an individual ad impression to be put up for bid in real-time. This is done through a programmatic <strong>on-the-spot auction</strong>, which is similar to how financial markets operate. RTB allows for Addressable Advertising; the ability to serve ads to consumers directly based on their demographic, psychographic, or behavioral attributes.</p>
<p>Many DSPs (Demand Side Platforms) act as agents for the advertisers and take part in the real-time auction on behalf of them. In order to enable real-time bidding and provide the advertisers with the clicks at the lowest price possible, DSPs develop their own machine learning algorithms using techniques such as <a href="https://en.wikipedia.org/wiki/Feature_hashing">hashing trick</a>, feature combinations, stochastic gradient descent etc.</p>
<h2>Motivation</h2>
<p>Like the standard practice in most of the data science use cases, whenever a new algorithm is developed, they are put into A/B test against the already existing algorithm in the production (at least for few days) in order to do determine which algorithm suits the business metrics better. </p>
<p>Due to the huge volume of bid requests (around a million bid requests per second), the amount of data collected during AB is in the order of 100 GBs. Python's Pandas library has basically all the functionality needed to do the offline analysis of the data collected in terms of CPCs, spend, clicks, CTR, AUC etc. </p>
<p>But, Pandas has a huge problem, it has to load all the dataset in memory in order to run some computations on it. From my experience, <strong>Pandas needs the RAM size to be 3 times the size of the dataset</strong> and it can not be run into a distributed environment as cluster a of machines. This is where <a href="https://spark.apache.org/">Apache Spark</a> is useful as it can process the datasets whose size is more than the size of the RAM. This blog will not cover the internals of Apache Spark and how it works rather I will jump to how the Pandas CTR Analysis code can be easily converted into spark analysis with few syntax changes.</p>
<h2>Migrating to Spark from Pandas</h2>
<p>In new versions, Spark started to support Dataframes which is conceptually equivalent to a dataframe in R/Python. Dataframe support in Spark has made it comparatively easy for users to switch to Spark from Pandas using a very similar syntax. In this section, I would jump to coding and show how the CTR analysis that is done in Pandas can be migrated to Spark. </p>
<p>Before I jump into the coding, I would like to introduce some of the keywords used in the code:</p>
<p><em>Effective CPC</em>: Total money spent / Total number of clicks</p>
<p><em>Label</em>: It is either 0 or 1 (1 signifies that the click happened and 0 is for no click)</p>
<p><em>Win Price</em>: The price paid to win the on-spot auction</p>
<p><em>Bid CPC</em>: The price the advertiser is willing to pay for the impression</p>
<p><em>CTR</em>: Click Through Rate = Total Number of Clicks / Total Number of Impressions</p>
<p><strong>How is Win Price different from Bid Price?</strong></p>
<p>If an exchange is using <a href="https://en.wikipedia.org/wiki/First-price_sealed-bid_auction">First Price Auction</a>, the win pice and the bid price is same but if the exchange is using <a href="https://en.wikipedia.org/wiki/Generalized_second-price_auction">Second Price Auction</a>, the advertizer with the highest bid price wins but it pays the price equivalent to the second highest bid price hence the win price is less than the bid price.</p>
<h3><strong>Setting up notebook and importing libraries</strong></h3>
<h5><strong>Pandas</strong></h5>
<pre class="code literal-block"><span></span><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</code></pre>


<h5><strong>Spark</strong></h5>
<pre class="code literal-block"><span></span><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'SPARK_HOME'</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"/home/spark-2.3.2-bin-hadoop2.7"</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'JAVA_HOME'</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"/home/jdk1.8.0_181"</span>
<span class="n">spark_home</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"SPARK_HOME"</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">spark_home</span> <span class="o">+</span> <span class="s2">"/python"</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">spark_home</span><span class="p">,</span> <span class="s2">"python/lib/py4j-0.10.7-src.zip"</span><span class="p">))</span>

<span class="kn">from</span>  <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>
<span class="kn">from</span> <span class="nn">pyspark.conf</span> <span class="kn">import</span> <span class="n">SparkConf</span>
<span class="n">CLUSTER_URL</span> <span class="o">=</span> <span class="s2">"spark://address:7077"</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span>
<span class="n">conf</span><span class="o">.</span><span class="n">setMaster</span><span class="p">(</span><span class="n">CLUSTER_URL</span><span class="p">)</span><span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="s2">"CTR Analysis"</span><span class="p">)</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">"spark.executor.memory"</span><span class="p">,</span> <span class="s2">"120g"</span><span class="p">)</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">version</span><span class="p">)</span>
</code></pre>


<h3><strong>Reading CSV File</strong></h3>
<h5><strong>Pandas</strong></h5>
<pre class="code literal-block"><span></span><code><span class="err">df = pd.read_csv(data_file_path, names=cols, error_bad_lines=False, warn_bad_lines=True, sep=',')</span>
</code></pre>


<h5><strong>Spark</strong></h5>
<pre class="code literal-block"><span></span><code><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StructType</span><span class="p">,</span> <span class="n">StructField</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">DoubleType</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">,</span> <span class="n">StringType</span>
<span class="n">file_location</span> <span class="o">=</span> <span class="s2">"hdfs://address:port/hadoop/dataNode/pyspark/data.csv"</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">file_location</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">schema</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">","</span><span class="p">)</span>
<span class="c1"># df = spark.read.csv(file_location, header=False, inferSchema=True, sep=",")</span>
<span class="c1"># df.cache()</span>
</code></pre>


<h3><strong>Cleaning data</strong></h3>
<h5><strong>Pandas</strong></h5>
<pre class="code literal-block"><span></span><code><span class="n">numeric_cols</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">'label','win_price','ctr','bid_cpc'</span><span class="o">]</span><span class="w"></span>
<span class="n">df</span><span class="o">[</span><span class="n">numeric_cols</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="o">[</span><span class="n">numeric_cols</span><span class="o">]</span><span class="p">.</span><span class="n">convert_objects</span><span class="p">(</span><span class="n">convert_numeric</span><span class="o">=</span><span class="k">True</span><span class="p">)</span><span class="w"></span>
<span class="n">df</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="k">True</span><span class="p">,</span><span class="w"> </span><span class="n">subset</span><span class="o">=</span><span class="n">numeric_cols</span><span class="p">)</span><span class="w"></span>
</code></pre>


<h5><strong>Spark</strong></h5>
<pre class="code literal-block"><span></span><code><span class="err">numeric_cols = ['label','win_price','ctr','bid_cpc']</span>
<span class="err">df = df.dropna(subset=numeric_cols)</span>
</code></pre>


<h3><strong>Calculating Spend, CTR, CPC Per Algo</strong></h3>
<h5><strong>Pandas</strong></h5>
<pre class="code literal-block"><span></span><code><span class="err">data = df.groupby(['algo']).agg({'win_price': np.sum, c_label:{'clicks':np.sum, 'wins':'count'}}).reset_index()</span>
<span class="err">data[('win_price','sum')] = data[('win_price','sum')] / 1000.</span>
<span class="err">data['ecpc'] = data[('win_price','sum')] / data[('label,'clicks')]</span>
<span class="err">data = pd.DataFrame(data.to_records())</span>
<span class="err">data.columns = ['',algo, 'spend', 'number of impressions', 'number of clicks', 'effective cpc']</span>
</code></pre>


<h5><strong>Spark</strong></h5>
<pre class="code literal-block"><span></span><code><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">udf</span>
<span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">as</span> <span class="nn">f</span>

<span class="k">def</span> <span class="nf">divide_by_1000</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">/</span><span class="mf">1000.0</span>

<span class="n">udfdivide_by_1000</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="n">divide_by_1000</span><span class="p">,</span> <span class="n">DoubleType</span><span class="p">())</span>

<span class="n">data_wins</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">'algo'</span><span class="p">])</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span> <span class="p">{</span><span class="s1">'win_price'</span><span class="p">:</span> <span class="s1">'sum'</span><span class="p">,</span> <span class="s1">'label: '</span><span class="n">count</span><span class="s1">'})</span>
<span class="n">data_clicks</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">'algo'</span><span class="p">])</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s1">'label: '</span><span class="nb">sum</span><span class="s1">'})</span>
<span class="c1"># print data_wins.columns</span>
<span class="c1"># print data_clicks.columns</span>
<span class="c1"># Rename the columns</span>
<span class="n">data_wins</span> <span class="o">=</span> <span class="n">data_wins</span><span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">"sum(win_price)"</span><span class="p">,</span> <span class="s2">"win_price"</span><span class="p">)</span><span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">"count(label)"</span><span class="p">,</span> <span class="s2">"wins"</span><span class="p">)</span>
<span class="c1">#     print data_wins.schema</span>
<span class="c1">#     data_wins['win_price'] = data_wins.win_price/1000.</span>
<span class="n">data_wins</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_wins</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">"win_price"</span><span class="p">,</span><span class="n">udfdivide_by_1000</span><span class="p">(</span><span class="s2">"win_price"</span><span class="p">)))</span>
<span class="n">data_clicks</span> <span class="o">=</span> <span class="n">data_clicks</span><span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">"sum(label)"</span><span class="p">,</span> <span class="s2">"clicks"</span><span class="p">)</span>
<span class="c1">#     print data_wins.columns</span>
<span class="c1"># print data_clicks.columns</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data_wins</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_clicks</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span> <span class="s1">'algo'</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">'inner'</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">"effective cpc"</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"win_price"</span><span class="p">)</span><span class="o">/</span><span class="mi">100</span><span class="p">))</span>
</code></pre>


<div id="disqus_thread"></div>

<script>
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//avoyage.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/adtech/" rel="tag">AdTech</a></li>
            <li><a class="tag p-category" href="../../categories/data-science/" rel="tag">Data Science</a></li>
            <li><a class="tag p-category" href="../../categories/machine-learning/" rel="tag">Machine Learning</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../gsoc-2017/" rel="prev" title="Google Summer of Code 2017">Previous post</a>
            </li>
            <li class="next">
                <a href="../building-machine-learning-data-pipeline-using-apache-spark/" rel="next" title="Building Machine Learning Data Pipeline using Apache Spark">Next post</a>
            </li>
        </ul></nav></aside></article><footer id="footer"><p>Contents © 2020         <a href="mailto:ayushpandey.iitkgp@gmail.com">Ayush Pandey</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </p>
            
        </footer>
</div>
    </div>
    <label for="sidebar-checkbox" class="sidebar-toggle"></label>
    
    
    
            <script src="../../assets/js/jquery.min.js"></script><script src="../../assets/js/moment-with-locales.min.js"></script><script src="../../assets/js/fancydates.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>
