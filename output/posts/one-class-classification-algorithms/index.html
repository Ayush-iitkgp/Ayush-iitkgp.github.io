<!DOCTYPE html>
<html prefix="" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>One-Class Classification Algorithms | Ayush Pandey</title>
<link href="../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/poole.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/lanyon.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://Ayush-iitkgp.github.io/posts/one-class-classification-algorithms/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Ayush Pandey">
<link rel="prev" href="../when-does-traditional-classification-algorithm-fail/" title="When does traditional classification algorithm fail?" type="text/html">
<link rel="next" href="../gsoc-2017/" title="Google Summer of Code 2017" type="text/html">
<meta property="og:site_name" content="Ayush Pandey">
<meta property="og:title" content="One-Class Classification Algorithms">
<meta property="og:url" content="http://Ayush-iitkgp.github.io/posts/one-class-classification-algorithms/">
<meta property="og:description" content="In my previous blog entry, I have put forth a case for when the traditional classification algorithms do not perform well i.e. when the training data has reasonable level of imbalance between the vari">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-01-22T12:13:21+05:30">
<meta property="article:tag" content="Application of Novelty Detection Algorithms to predict Electricity Theft">
<meta property="article:tag" content="Data Science">
<meta property="article:tag" content="Machine Learning">
</head>
<body class="theme-base-0d">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>
    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
            styles, `#sidebar-checkbox` for behavior. -->
    <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox"><!-- Toggleable sidebar --><div class="sidebar" id="sidebar">
        <!---<div class="sidebar-item">
            <p>A reserved <a href="https://getnikola.com" target="_blank">Nikola</a> theme that places the utmost gravity on content with a hidden drawer. Made by <a href="https://twitter.com/mdo" target="_blank">@mdo</a> for Jekyll,
            ported to Nikola by <a href="https://twitter.com/ralsina" target="_blank">@ralsina</a>.</p>
        </div> -->
        
    <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../../blog">Blog</a>
        <a class="sidebar-nav-item" href="https://drive.google.com/file/d/1ivW6csbp9xI5cuGABnqIOriUDvFyNB88/view?usp=sharing">Resume</a>
        <a class="sidebar-nav-item" href="https://drive.google.com/drive/folders/0B2oOdWdSJWa1RWRsQWdCZ25LUXc">University Projects</a>
        <a class="sidebar-nav-item" href="../gsoc-2017/">GSoC'17</a>
        <a class="sidebar-nav-item" href="../is-it-the-end-or-another-beginning/">GSoC'16</a>
    
    
    </nav>
</div>

    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          
    <h3 id="brand" class="masthead-title">
      <a href="http://Ayush-iitkgp.github.io/" title="Ayush Pandey" rel="home">Ayush Pandey</a>
    </h3>

        </div>
      </div>

      <div class="container content" id="content">
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="post-title p-name entry-title" itemprop="headline name"><a href="." class="u-url">One-Class Classification Algorithms</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">Ayush Pandey</span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="post-date published dt-published" datetime="2017-01-22T12:13:21+05:30" itemprop="datePublished" title="2017-01-22 12:13">2017-01-22 12:13</time></a></p>
        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>In my <a href="../when-does-traditional-classification-algorithm-fail/">previous blog entry</a>, I have put forth a case for when the traditional classification algorithms do not perform well i.e. when the training data has reasonable level of imbalance between the various classes. The <a href="https://drive.google.com/file/d/0B2oOdWdSJWa1NF8ySXhvR1ZvODA/view?usp=sharing">problem statement</a> that I was trying to solve had a similar problem of the skewed distribution of the training data. <strong>The power company had only created the database of fraudulent customers</strong>. </p>
<p>As I mentioned in my previous blog post, there are 2 methods to tackle the case when we have data from only one class:</p>
<ol>
<li>
<p>The first method is an obvious approach to generate an artificial second class and proceed with traditional classification algorithms.</p>
</li>
<li>
<p>The second one is to modify the existing classification algoriths to learn on the data from only one class. These algorithms are called "one-class classfication algorithms" and include one-class SVM, One-Class K-Means, One-Class K-Nearest Neighbor, and One-Class Gaussian.</p>
</li>
</ol>
<p>In this blog entry, I will elaborate on the second method and explain the mathematics behind the one-class classification algorithms and how it improves over the traditional classification algorithms. </p>
<h3>Fundamental difference between Binary and One Class Classification Algorithms</h3>
<p>Binary classification algorithms are <em>discriminatory in nature</em>, since they learn to discriminate between classes using all data classes to create a hyperplane(as seen in the fig. below) and use the hyperplane to label a new sample. In case of imbalance between the classes, the discriminatory methods can not be used to their full potential, since by their very nature, they rely on data from all classes to build the hyperplane that separate the various classes.</p>
<p></p>
<center>
<img src="../../images/binaryClassification.png" alt="Binary Classification Hyperplane" height="200px" width="375px" border="1px" style="margin: 0px 20px">Binary Classification Hyperplane</center>
<br><p>Where as the one-class algorithms are based on <em>recognition</em> since their aim is to recognize data from a particular class, and reject data from all other classes. This is accomplished by creating a boundary that encompasses all the data belonging to the target class within itself, so when a new sample arrives the algorithm only has to check whether it lies within the boundary or outside and accordingly classify the sample as belonging to the target class or the outlier.</p>
<p></p>
<center>
<img src="../../images/oneClassClassification.png" alt="One-Class Classification Boundary" height="200px" width="375px" border="1px" style="margin: 0px 20px">One-Class Classification Boundary</center>
<br><h3>Mathematics behind different One-Class Classification Algorithms</h3>
<p>In this section, I will explain the mathematics behind different one-class machine learning algorithms by taking a cue from <a href="http://file.scirp.org/pdf/JBiSE20100300003_45072138.pdf">this research paper</a>.</p>
<h4><strong>One-Class Gaussian</strong></h4>
<p>This is basically a density estimation model. It assumes that the training data are the samples from the <em>Multivariate Normal Population</em>, therefore for a test sample (say z) having n-feaures, the probability of it belonging to the target class can be calculated as follows:</p>
<p></p>
<center><img src="../../images/one-classGaussian.png" alt="one-class gaussian" height="100px" width="375px" border="1px" style="margin: 0px 20px"></center>
<br><p>where the parameters μ and Σ are the <em>poputation mean and covariance</em>. Hence, the objective function of this machine learning algorithm is to determine the estimates for μ and Σ. Using the <strong>method of maximum likelihood estimator</strong>, we can show that the sample mean and sample covariance are the unbiased and consistent estimators for population mean and variance respectively. Hence, once we calculate the probability p(z), we can set a threshold to determine whether a new sample is outlier or not.</p>
<h4><strong>One-Class Kmeans</strong></h4>
<p>In this method, we first classify the training data into k clusters (which is chosen as per our requirements). Then, for a new sample (say z), the distance <em>d(z)</em> is calculated as the minimum distance of the sample from the centroid of all the k clusters. Now if <em>d(z)</em> is less than a particular thereshold (which is again chosen as per our requirements), then the sample belongs to the target class otherwise it is classified as the outlier.</p>
<h4><strong>One-Class K-Nearest Neighbor</strong></h4>
<p>Let us take note of some notation before we understand the mathematics behind the algorithm.</p>
<p><em>d(z,y)</em> : distance between two samples z and y</p>
<p><em>NN(y)</em> : Nearest Neighbor of sample y</p>
<p>Now given a test sample z, we find the nearest neighbor of z from the training data (which is NN(z) = y) and the nearest neighbor of y (which is NN(y)). Now the rule is to classify z as belonging to the target class when:</p>
<p></p>
<center><img src="../../images/K-NearestNeighbor.png" alt="K-Nearest Neighbor" height="100px" width="375px" border="1px" style="margin: 0px 20px"></center>
<br><p>where the default value of δ is 1 but can be chosen to satisfy our requirements.</p>
<p>In my next blog entry, I will try to explain the most widely used one-class classification algorithm i.e. one-class support vector machines. Stay tuned !</p>
<p>Thank you very much for making it this far.</p>
<div id="disqus_thread"></div>
<script>
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//avoyage.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/application-of-novelty-detection-algorithms-to-predict-electricity-theft/" rel="tag">Application of Novelty Detection Algorithms to predict Electricity Theft</a></li>
            <li><a class="tag p-category" href="../../categories/data-science/" rel="tag">Data Science</a></li>
            <li><a class="tag p-category" href="../../categories/machine-learning/" rel="tag">Machine Learning</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../when-does-traditional-classification-algorithm-fail/" rel="prev" title="When does traditional classification algorithm fail?">Previous post</a>
            </li>
            <li class="next">
                <a href="../gsoc-2017/" rel="next" title="Google Summer of Code 2017">Next post</a>
            </li>
        </ul></nav></aside></article><footer id="footer"><p>Contents © 2022         <a href="mailto:ayushpandey.iitkgp@gmail.com">Ayush Pandey</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </p>
            
        </footer>
</div>
    </div>
    <label for="sidebar-checkbox" class="sidebar-toggle"></label>
    
    
    
            <script src="../../assets/js/jquery.min.js"></script><script src="../../assets/js/moment-with-locales.min.js"></script><script src="../../assets/js/fancydates.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>

