<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ayush Pandey (Posts about posts)</title><link>http://Ayush-iitkgp.github.io/</link><description></description><atom:link href="http://Ayush-iitkgp.github.io/categories/cat_posts.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Tue, 20 Jun 2023 22:27:00 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Journey of Reflection: A walk to Camino</title><link>http://Ayush-iitkgp.github.io/posts/journey-of-reflection-a-walk-to-camino/</link><dc:creator>Ayush Pandey</dc:creator><description>&lt;p&gt;&lt;strong&gt;The Camino de Santiago is not just a walk; it is a journey of the soul, where every step carries the weight of countless stories and the promise of personal transformation&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;That is how I would describe my experience of walking the Camino.&lt;/p&gt;
&lt;p&gt;It was during a call with a very close friend of mine in the midst of long and very hard Berlin winter that I decided to hike the Camino De Santiago. People have different motivations for the walk, mine was to establish a deeper connection with myself and push myself physically to try to achieve long distance walking of around 130 km in 5 days that I had never ever conceived in my wildest imaginations.&lt;/p&gt;
&lt;p&gt;In this blogpost, I would share the learnings and realizations from the experience I had :)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Preparation is the key&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since, I had never imagined walking 130 km, taking on this challenge meant I had to prepare myself physically and mentally.&lt;/p&gt;
&lt;p&gt;I picked up new a new sports hobby high intensity indoor cycling (spinning) and regularly attended HIIT classes. I also went hiking for 20 km or more on the outskirts of Berlin almost every weekend leading upto the Camino.&lt;/p&gt;
&lt;p&gt;In order to train mentally, I read and talked to the people about their Camino experiences, practiced being open to embrace the unknowns and most importantly set aside time for myself to embrace the solitude allow myself the space to process emotions, find inspiration, and foster personal growth.&lt;/p&gt;
&lt;p&gt;Last but not the least, taking care of the logistical planning immensely made the experience more pleasureful.&lt;/p&gt;
&lt;p&gt;See you next time till then keep laughing, spreading happiness and the most importantly living this journey called life!!!! &lt;/p&gt;
&lt;div id="disqus_thread"&gt;&lt;/div&gt;
&lt;script&gt;
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//avoyage.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;
&lt;noscript&gt;Please enable JavaScript to view the &lt;a href="https://disqus.com/?ref_noscript" rel="nofollow"&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</description><guid>http://Ayush-iitkgp.github.io/posts/journey-of-reflection-a-walk-to-camino/</guid><pubDate>Mon, 12 Jun 2023 16:14:47 GMT</pubDate></item><item><title>Learnings from a month of being an Assistant Tech Lead!</title><link>http://Ayush-iitkgp.github.io/posts/learnings-from-a-month-of-being-a-tech-lead/</link><dc:creator>Ayush Pandey</dc:creator><description>&lt;p&gt;Today is exactly a month since I was given the responsibility of co-leading a small team of software developers at &lt;a href="https://www.adacta-fintech.com/"&gt;Adacta Fintech&lt;/a&gt; to develop insurance solutions for our clients. It was intimidating at the beginning not just because I was taking this responsibility for the first time but also, it came at a time when the Tech Lead and the Product Manager were on vacation simultaneously and the senior developers in the team were moving to other opportunities in their career.&lt;/p&gt;
&lt;p&gt;As they say, they never waste a crisis, I took this challenge as an opportunity to hone my skills as a Technical Lead. In this blog post, I will share some of the learnings and realizations I went through in this period:&lt;/p&gt;
&lt;p&gt;** 1. Preparation is the key**&lt;/p&gt;
&lt;p&gt;At first, it was intimidating to know that I needed to talk to different stakeholders such as Business Analysts, Product Manager, and Developers. Also provide estimations, write technical notes, organize sprints, and take care of JIRA tasks all at the same time instead of just being in my shell and programming. Thanks to my product manager and my senior lead, we were able to organize multiple sessions on how to handle the different responsibilities, how to articulate the business aspects of the project and find a correlation between the technical and the business sides of the project. While it is never enough, I did build some confidence over the course of these multiple sessions and I highly advise it for anyone aiming to transition into a Technical Lead role.&lt;/p&gt;
&lt;p&gt;** 2. Imposter Syndrome is natural, accept it and be honest about it with your colleagues**&lt;/p&gt;
&lt;p&gt;As &lt;a href="https://www.linkedin.com/in/rpandey1234/"&gt;Rahul Pandey&lt;/a&gt; points out in his &lt;a href="https://www.youtube.com/watch?v=F_CLhDvtYrs&amp;amp;ab_channel=Codementor"&gt;talk&lt;/a&gt;, every time we are faced with a new challenge, the feeling of being lost is something that every engineer faces. The fact that I accepted it helped me immensely to become comfortable with the new position I was assigned to. One of the first things I would do at the start of the meeting is to let the other members know about my situation and this gave me more confidence to ask them questions if I felt I did not exactly understand something. As a developer, you could still get away with not exactly understanding the business implications of what you are doing as you can always fall back to your Tech lead to ask him in more detail but when you are the one leading the team, you no longer have that leverage as the rest of the team depends on you. In fact, I am glad people around me appreciated the fact that I was honest to them and they spent more time explaining things to me than they would have preferred to.&lt;/p&gt;
&lt;p&gt;** 3. Communicating your vision to the team is super important**&lt;/p&gt;
&lt;p&gt;As a lead, it is super important that your team becomes aligned with the vision you have for the product and your people. It not only helps the development move in a direction as per the needs of the business but also keeps your team members motivated and engaged. One of the first things I did when I took charge was to let the team know what development trajectory we needed to take to meet the business needs of the company. At the same time, I also let them know about my plans to further their learning and help them improve as a developer in the next month. This helped us to measure our progress at the end of the month and we got valuable suggestions on how to improve as a team.&lt;/p&gt;
&lt;p&gt;** 4. Technical Debt exists, accept it, and do not fix it yourself**&lt;/p&gt;
&lt;p&gt;I joined my current team when a substantial portion of the project was already built. As a developer, I used to be very aware of not introducing technical debt in my code. Now as a tech lead, I had to review other people's code and had the opportunity to dive deeper into the aspects of the code that I had not encountered. As a consequence, I encountered the technical debt in the code, I was tempted to go in and fix errors myself but that meant sometimes I was overworked and burnt-out. Realizing the approach was not feasible, I started with the process of continuous feedback having small calls with the members to let them know what better could have been done whenever I encountered any such issue. We also agreed with the team to not introduce any technical debt in a small part of the project to start with. I started to lead by example and asked members to review my code so they could follow some of my coding practices to avoid introducing technical debt.&lt;/p&gt;
&lt;p&gt;** 5. Knowledge Transfer will reduce your work by many-fold and increase the efficiency of the team**&lt;/p&gt;
&lt;p&gt;Coming from an open-source community, I have always been excited to share my knowledge and learn from other people as much as I can. Because most of the developers in the team were relatively new to the company, I was all of the sudden spending almost all my time answering the developer's queries and there was a pattern in the questions. So, I decided to aggressively organize knowledge transfer sessions not only from technical people in the team but also from the business people. We started to record the sessions and write notes not just for the present members of the team who were absent but potentially for the future members of the team. This helped us avoid single point of failure as all of us were partially aware of the part of the project that we were not working on. It also motivated members to dive deep into the development so they could present some of the work they were doing.&lt;/p&gt;
&lt;p&gt;** 6. Do not burn yourself**&lt;/p&gt;
&lt;p&gt;It is natural that I wanted to prove that I am fit for the new responsibilities that are assigned to me. As a consequence, I started to code and write emails to different stakeholders outside my work hours. While I could do it for the first few weeks but I started to burn out after some time. My suggestion is to accept that it’s exhausting and pace yourself. Chill, Watch movies, Play Cricket, Work out. Find what works for you. Get the steam out :-)&lt;/p&gt;
&lt;p&gt;Let me know what you think about my blog, I am looking forward to your suggestions and comments.&lt;/p&gt;
&lt;p&gt;See you next time till then keep laughing, spreading happiness and most importantly living this journey called life!!!!&lt;/p&gt;
&lt;div id="disqus_thread"&gt;&lt;/div&gt;
&lt;script&gt;
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//avoyage.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;
&lt;noscript&gt;Please enable JavaScript to view the &lt;a href="https://disqus.com/?ref_noscript" rel="nofollow"&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</description><guid>http://Ayush-iitkgp.github.io/posts/learnings-from-a-month-of-being-a-tech-lead/</guid><pubDate>Tue, 18 Jan 2022 16:14:47 GMT</pubDate></item><item><title>Applying Machine Learning in Ad Tech</title><link>http://Ayush-iitkgp.github.io/posts/applying-machine-learning-in-adtech/</link><dc:creator>Ayush Pandey</dc:creator><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In recent years, programmatic advertising is been taking over the online advertisement industry. Many companies referred to as DSP (Demand Side Platform) compete for the same ad-slot on the internet. The success of the DSPs to deliver the values to the advertisers is evaluated on the below two criteria:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;High Click Through Rate = Number of clicks/ Number of ads shown&lt;/li&gt;
&lt;li&gt;High Conversion Rate = Number of conversions (such as a purchase)/ Number of ads shown&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To achieve high CTR and Conversion Rates, DSPs heavily rely on using Artificial Intelligence techniques and develop their in-house Machine Learning algorithms. The problem of applying Machine Learning in Adtech is different from the standard problem in many sense. Google's paper &lt;a href="https://ai.google/research/pubs/pub41159"&gt;&lt;em&gt;Ad Click Prediction: a view from the Trenches&lt;/em&gt;&lt;/a&gt; and Facebook's paper &lt;a href="https://research.fb.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/"&gt;&lt;em&gt;Practical Lessons from Predicting Clicks on Ads at Facebbok&lt;/em&gt;&lt;/a&gt; have discussed in details the lessons learned while building an AI for the adtech industry. In the remaining blog post, I will try to summarize the intricacies of applying ML in adtech and how it is tackled in general:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Large size of the training vector:&lt;/strong&gt; Every feature in the ML model is a categorical feature and encoding them into numerical feature explodes the size of the training vector to the order of billions. For example, one of the most important features in the ML model is the &lt;em&gt;publisher&lt;/em&gt; website where the ad would be displayed which is a categorical feature and there are millions of publishers so using one-hot encoding would result in a training vector of million entries.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Skewness of the training data:&lt;/strong&gt; Typically, CTRs are much lower than 50% (generally, CTR is around 1-2%)which means that positive examples (clicks) are relatively rare, hence is the problem of skewness in the training data is introduced.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rapid changes in the online ad landscape:&lt;/strong&gt; The adtech domain is a very dynamic environment where the data distribution changes over time. Facebook conducted an experiment where they trained the model on one day of data and evaluated on the six consecutive days. The results showed that the performance of the model decreases as the delay between the training and test set increases. Thus, it is very important to update the model every few hours to keep it very real-time&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Per-coordinate learning rate:&lt;/strong&gt; In most of the standard ML problems, the learning rate is a constant. In adtech, there is a huge imbalance of the number of training instances on each feature. For example, a famous publisher such as cnn.com will be having more users thus more ad-slots compared to a very unknown one thus, our training data will have a huge number of training instances for cnn.com. Therefore, we want to decrease the learning rate for the coordinate as its frequency in the training data increases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Using Progressive Validation rather than cross-validation:&lt;/strong&gt; Validating the model on the data set which lags the train set by hours or days is not a good idea as we discussed above that the nature of the dataset changes with time. The &lt;em&gt;online log loss&lt;/em&gt; is instead a good proxy for the model performance because it measures the performance only on the most recent data before we train on it, this is exactly analogous to what happens when the model is in production. This also ensures that we can use 100% of our data for both training and testing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relative changes in the metric compared to Absolute metric:&lt;/strong&gt; Click rates vary from country to country and from ad-slot to ad-slot and therefore the metrics change over the course of a single day. Google experiments indicate that the relative changes(compared to a baseline model) are much more stable over time, therefore, a relative change in logloss is a better metric than the average log loss. We also take care only to compare metrics computed from exactly the same data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Segmented performance metrics instead of aggregated metrics:&lt;/strong&gt; One of the things that we have to take care while analyzing the performance of the models in adtech is that the aggregated performance metrics may hide effects that are specific to certain sub-populations of the data. For example, a high CTR may, in fact, be caused by a mix of low and high CTR from different ad exchanges. This makes it critical to visualize the performance metrics not only on the aggregate data but also on the various slicing of the data such as per ad exchange, per adgroup, per device type, per publisher.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This blog is the aggregation of all the lessons learnt while working as Data Scientist in the AdTech company. Do let me know if you have any additional comments. &lt;/p&gt;
&lt;p&gt;Do you have any questions?&lt;/p&gt;
&lt;p&gt;Ask your questions in the comments below and I will do my best to answer.&lt;/p&gt;
&lt;div id="disqus_thread"&gt;&lt;/div&gt;
&lt;script&gt;
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//avoyage.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;
&lt;noscript&gt;Please enable JavaScript to view the &lt;a href="https://disqus.com/?ref_noscript" rel="nofollow"&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</description><category>AdTech</category><category>Data Science</category><category>Machine Learning</category><guid>http://Ayush-iitkgp.github.io/posts/applying-machine-learning-in-adtech/</guid><pubDate>Sun, 07 Apr 2019 20:43:21 GMT</pubDate></item><item><title>Building Machine Learning Data Pipeline using Apache Spark</title><link>http://Ayush-iitkgp.github.io/posts/building-machine-learning-data-pipeline-using-apache-spark/</link><dc:creator>Ayush Pandey</dc:creator><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://Ayush-iitkgp.github.io/posts/building-machine-learning-data-pipeline-using-apache-spark/"&gt;Apache Spark&lt;/a&gt; is increasingly becoming popular in the field of Data Sciences because of its ability to deal with the huge datasets and the capability to  run computations in memory which is particularly useful in iterative tasks such as the training step of the Machine Learning algorithm. As part of the Data Engine team at &lt;a href="https://www.sprinklr.com/"&gt;Sprinklr&lt;/a&gt;, I had some experience building the data processing pipeline in Spark. In this blog post, I will try to summarize my learning in simpler, easy to understand terms along with the python code.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q. Why is Apache Spark a suitable tool for building the ML data pipeline?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ans.&lt;/strong&gt; Few years ago, &lt;a href="https://scikit-learn.org/stable/"&gt;scikit-learn&lt;/a&gt; came up with the idea of data pipeline but with the advent of big data, it became very problematic to scale. Spark's data pipeline concept is mostly inspired by the scikit-learn project. It provides the APIs for machine learning algorithms which make it easier to combine multiple algorithms into a single pipeline, or workflow.&lt;/p&gt;
&lt;p&gt;Now, I will introduce the key concepts used in the Pipeline API:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DataFrame:&lt;/strong&gt; It is basically a data structure for storing the data in-memory in a highly efficient way. Dataframe in Spark is conceptually equivalent to a dataframe in R/Python. It can store  different data types such a string, vectors, true labels, and predictions. Dataframes can be created from the csv, json and many different file formats stored on the local filesystem, Hadoop HDFS or cloud environment such as AWS S3.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transformer:&lt;/strong&gt; It is a method or an algorithm which can transform one DataFrame into another DataFrame. It includes SQL statements, feature transformers and learned ML models. While defining a transformer, you have to specify the column it would operate on and the output column it would append to the input DataFrame. Technically, a Transformer implements a method &lt;strong&gt;transform()&lt;/strong&gt;. E.g., a SQL &lt;em&gt;select&lt;/em&gt; statement which would return a new dataframe with only required columns. Another example is a &lt;em&gt;trained ML model&lt;/em&gt; which turns a dataframe with feature vectors into a dataframe with predictions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Estimator:&lt;/strong&gt; It is an algorithm which can be fit on a DataFrame to produce a Transformer.  Technically, an Estimator implements a method &lt;strong&gt;fit()&lt;/strong&gt; which accepts a DataFrame and produces a &lt;strong&gt;Model&lt;/strong&gt; which is a Transformer. For example, a machine learning algorithm is an Estimator which trains on a DataFrame and produces a trained model which is a transformer as it can transform a feature vector into predictions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Parameter:&lt;/strong&gt; These are the hyperparameters used during cross-validation phase of the ML pipeline.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pipeline:&lt;/strong&gt; A Pipeline is a sequence of PipelineStage (Transformers and Estimators)together to be running in a particular order to specify a Machine Learning workflow. &lt;strong&gt;A Pipeline’s stages are specified as an ordered array&lt;/strong&gt;. For example predicting the price of a house given it's breadth, length, location and age involve several stages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remove the data points which have all columns as null value - Transformer&lt;/li&gt;
&lt;li&gt;Create a new feature &lt;strong&gt;area&lt;/strong&gt; - Transformer&lt;/li&gt;
&lt;li&gt;Learn a prediction model using the feature vectors and the actual price - Estimator&lt;/li&gt;
&lt;li&gt;Use the learned model to predict the prices - Transformer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A Pipeline is an &lt;em&gt;Estimator&lt;/em&gt; in itself. Thus, after a Pipeline’s fit() method is called, it produces a &lt;em&gt;PipelineModel&lt;/em&gt;, which is a Transformer. &lt;/p&gt;
&lt;p&gt;PipelineModel has the same number of stages as the original Pipeline, &lt;strong&gt;but all Estimators in the original Pipeline have become Transformers&lt;/strong&gt;. This PipelineModel is used at test time. &lt;/p&gt;
&lt;p&gt;Let's dive into the Python code using an example mentioned in the Spark's doc &lt;a href="https://spark.apache.org/docs/latest/ml-pipeline.html#example-pipeline"&gt;here&lt;/a&gt; where we are trying to classify a line of text into 0 or 1:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1: Create a DataFrame&lt;/strong&gt;&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="c1"&gt;# Some import statements&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.ml&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.ml.classification&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.ml.feature&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;HashingTF&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Tokenizer&lt;/span&gt;

&lt;span class="c1"&gt;# DataFrame could be created from a csv file or any other sources &lt;/span&gt;
&lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createDataFrame&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"a b c d e spark"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"b d"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"spark f g h"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"hadoop mapreduce"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"id"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"text"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"label"&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Let's also create a test dataset which we will use later&lt;/span&gt;
&lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createDataFrame&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"spark i j k"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"l m n"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"spark hadoop spark"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"apache hadoop"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"id"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"text"&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 2: Specify the transformers and the estimators of the pipeline&lt;/strong&gt;&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Tokenizer&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;is&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;transformer&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;which&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;would&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;convert&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;into&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;using&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;space&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;delimeter&lt;/span&gt;
&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"text"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;outputCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"words"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;HashingTF&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;is&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;again&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;transformer&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;which&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;takes&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;"words as an input and creates a new column of a vector"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="n"&gt;hashingTF&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;HashingTF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getOutputCol&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;outputCol&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;"features"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="p"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Logistic&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Regression&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;is&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;an&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Estimator&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;which&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;would&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;take&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;"features"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;and&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;"label"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;an&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;input&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;and&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;create&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;trained&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;
&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxIter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mh"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;regParam&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 3: Create the pipeline using the transformers and the estimators defined in step 2&lt;/strong&gt;&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 4: Call the fit() method on the pipeline to create a PipelineModel&lt;/strong&gt;&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;model = pipeline.fit(training)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 5: Use the PipelineModel to do the predictions of the test dataset&lt;/strong&gt;&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="nv"&gt;prediction&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;model&lt;/span&gt;.&lt;span class="nv"&gt;transform&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;test&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;selected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;prediction&lt;/span&gt;.&lt;span class="nv"&gt;select&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"id"&lt;/span&gt;,&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"text"&lt;/span&gt;,&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"probability"&lt;/span&gt;,&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"prediction"&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;selected&lt;/span&gt;.&lt;span class="k"&gt;show&lt;/span&gt;&lt;span class="ss"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;One of the big benefits of the Machine Learning Data Pipeline in Spark is hyperparameter optimization which I would try to explain in the next blog post. I hope this blog would help you in getting started with Spark for building ML data pipelines. &lt;/p&gt;
&lt;div id="disqus_thread"&gt;&lt;/div&gt;
&lt;script&gt;
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//avoyage.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;
&lt;noscript&gt;Please enable JavaScript to view the &lt;a href="https://disqus.com/?ref_noscript" rel="nofollow"&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</description><category>Data Science</category><category>Machine Learning</category><guid>http://Ayush-iitkgp.github.io/posts/building-machine-learning-data-pipeline-using-apache-spark/</guid><pubDate>Tue, 19 Mar 2019 20:43:21 GMT</pubDate></item><item><title>Click Through Rate Analysis using Spark</title><link>http://Ayush-iitkgp.github.io/posts/ctr-analysis-using-spark/</link><dc:creator>Ayush Pandey</dc:creator><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In recent years, programmatic advertising is been taking over the online advertisement industry. To enable automatic selling and purchasing ad impressions between advertisers and publishers through real-time auctions, Real-Time Bidding (RTB) is quickly becoming the leading method.&lt;/p&gt;
&lt;p&gt;In contrast to the traditional online ad market, where a certain amount of impressions is sold at a fixed rate, RTB allows advertisers to bid each impression individually in real time at a cost based on impression-level features. Real-time Bidding (RTB) is a way of transacting media that allows an individual ad impression to be put up for bid in real-time. This is done through a programmatic &lt;strong&gt;on-the-spot auction&lt;/strong&gt;, which is similar to how financial markets operate. RTB allows for Addressable Advertising; the ability to serve ads to consumers directly based on their demographic, psychographic, or behavioral attributes.&lt;/p&gt;
&lt;p&gt;Many DSPs (Demand Side Platforms) act as agents for the advertisers and take part in the real-time auction on behalf of them. In order to enable real-time bidding and provide the advertisers with the clicks at the lowest price possible, DSPs develop their own machine learning algorithms using techniques such as &lt;a href="https://en.wikipedia.org/wiki/Feature_hashing"&gt;hashing trick&lt;/a&gt;, feature combinations, stochastic gradient descent etc.&lt;/p&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Like the standard practice in most of the data science use cases, whenever a new algorithm is developed, they are put into A/B test against the already existing algorithm in the production (at least for few days) in order to do determine which algorithm suits the business metrics better. &lt;/p&gt;
&lt;p&gt;Due to the huge volume of bid requests (around a million bid requests per second), the amount of data collected during AB is in the order of 100 GBs. Python's Pandas library has basically all the functionality needed to do the offline analysis of the data collected in terms of CPCs, spend, clicks, CTR, AUC etc. &lt;/p&gt;
&lt;p&gt;But, Pandas has a huge problem, it has to load all the dataset in memory in order to run some computations on it. From my experience, &lt;strong&gt;Pandas needs the RAM size to be 3 times the size of the dataset&lt;/strong&gt; and it can not be run into a distributed environment as cluster a of machines. This is where &lt;a href="https://spark.apache.org/"&gt;Apache Spark&lt;/a&gt; is useful as it can process the datasets whose size is more than the size of the RAM. This blog will not cover the internals of Apache Spark and how it works rather I will jump to how the Pandas CTR Analysis code can be easily converted into spark analysis with few syntax changes.&lt;/p&gt;
&lt;h2&gt;Migrating to Spark from Pandas&lt;/h2&gt;
&lt;p&gt;In new versions, Spark started to support Dataframes which is conceptually equivalent to a dataframe in R/Python. Dataframe support in Spark has made it comparatively easy for users to switch to Spark from Pandas using a very similar syntax. In this section, I would jump to coding and show how the CTR analysis that is done in Pandas can be migrated to Spark. &lt;/p&gt;
&lt;p&gt;Before I jump into the coding, I would like to introduce some of the keywords used in the code:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Effective CPC&lt;/em&gt;: Total money spent / Total number of clicks&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Label&lt;/em&gt;: It is either 0 or 1 (1 signifies that the click happened and 0 is for no click)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Win Price&lt;/em&gt;: The price paid to win the on-spot auction&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Bid CPC&lt;/em&gt;: The price the advertiser is willing to pay for the impression&lt;/p&gt;
&lt;p&gt;&lt;em&gt;CTR&lt;/em&gt;: Click Through Rate = Total Number of Clicks / Total Number of Impressions&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How is Win Price different from Bid Price?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If an exchange is using &lt;a href="https://en.wikipedia.org/wiki/First-price_sealed-bid_auction"&gt;First Price Auction&lt;/a&gt;, the win pice and the bid price is same but if the exchange is using &lt;a href="https://en.wikipedia.org/wiki/Generalized_second-price_auction"&gt;Second Price Auction&lt;/a&gt;, the advertizer with the highest bid price wins but it pays the price equivalent to the second highest bid price hence the win price is less than the bid price.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Setting up notebook and importing libraries&lt;/strong&gt;&lt;/h3&gt;
&lt;h5&gt;&lt;strong&gt;Pandas&lt;/strong&gt;&lt;/h5&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;h5&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;/h5&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'SPARK_HOME'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"/home/spark-2.3.2-bin-hadoop2.7"&lt;/span&gt;
&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'JAVA_HOME'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"/home/jdk1.8.0_181"&lt;/span&gt;
&lt;span class="n"&gt;spark_home&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"SPARK_HOME"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;spark_home&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;"/python"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;spark_home&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"python/lib/py4j-0.10.7-src.zip"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt;  &lt;span class="nn"&gt;pyspark&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.conf&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SparkConf&lt;/span&gt;
&lt;span class="n"&gt;CLUSTER_URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"spark://address:7077"&lt;/span&gt;
&lt;span class="n"&gt;conf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SparkConf&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setMaster&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CLUSTER_URL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setAppName&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"CTR Analysis"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"spark.executor.memory"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"120g"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;version&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;&lt;strong&gt;Reading CSV File&lt;/strong&gt;&lt;/h3&gt;
&lt;h5&gt;&lt;strong&gt;Pandas&lt;/strong&gt;&lt;/h5&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;df = pd.read_csv(data_file_path, names=cols, error_bad_lines=False, warn_bad_lines=True, sep=',')
&lt;/pre&gt;&lt;/div&gt;

&lt;h5&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;/h5&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql.types&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StructType&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StructField&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql.types&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DoubleType&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;IntegerType&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StringType&lt;/span&gt;
&lt;span class="n"&gt;file_location&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"hdfs://address:port/hadoop/dataNode/pyspark/data.csv"&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;file_location&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;","&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# df = spark.read.csv(file_location, header=False, inferSchema=True, sep=",")&lt;/span&gt;
&lt;span class="c1"&gt;# df.cache()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;&lt;strong&gt;Cleaning data&lt;/strong&gt;&lt;/h3&gt;
&lt;h5&gt;&lt;strong&gt;Pandas&lt;/strong&gt;&lt;/h5&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="n"&gt;numeric_cols&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;'label','win_price','ctr','bid_cpc'&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;numeric_cols&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;numeric_cols&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convert_objects&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;convert_numeric&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;subset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;numeric_cols&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;h5&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;/h5&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;numeric_cols = ['label','win_price','ctr','bid_cpc']
df = df.dropna(subset=numeric_cols)
&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;&lt;strong&gt;Calculating Spend, CTR, CPC Per Algo&lt;/strong&gt;&lt;/h3&gt;
&lt;h5&gt;&lt;strong&gt;Pandas&lt;/strong&gt;&lt;/h5&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;data = df.groupby(['algo']).agg({'win_price': np.sum, c_label:{'clicks':np.sum, 'wins':'count'}}).reset_index()
data[('win_price','sum')] = data[('win_price','sum')] / 1000.
data['ecpc'] = data[('win_price','sum')] / data[('label,'clicks')]
data = pd.DataFrame(data.to_records())
data.columns = ['',algo, 'spend', 'number of impressions', 'number of clicks', 'effective cpc']
&lt;/pre&gt;&lt;/div&gt;

&lt;h5&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;/h5&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql.functions&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;udf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql.functions&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;f&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;divide_by_1000&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;1000.0&lt;/span&gt;

&lt;span class="n"&gt;udfdivide_by_1000&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;udf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;divide_by_1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DoubleType&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="n"&gt;data_wins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;'algo'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'win_price'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'sum'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'label: '&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="s1"&gt;'})&lt;/span&gt;
&lt;span class="n"&gt;data_clicks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;'algo'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;'label: '&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="s1"&gt;'})&lt;/span&gt;
&lt;span class="c1"&gt;# print data_wins.columns&lt;/span&gt;
&lt;span class="c1"&gt;# print data_clicks.columns&lt;/span&gt;
&lt;span class="c1"&gt;# Rename the columns&lt;/span&gt;
&lt;span class="n"&gt;data_wins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_wins&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withColumnRenamed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"sum(win_price)"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"win_price"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withColumnRenamed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"count(label)"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"wins"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;#     print data_wins.schema&lt;/span&gt;
&lt;span class="c1"&gt;#     data_wins['win_price'] = data_wins.win_price/1000.&lt;/span&gt;
&lt;span class="n"&gt;data_wins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_wins&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withColumn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"win_price"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;udfdivide_by_1000&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"win_price"&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;data_clicks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_clicks&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withColumnRenamed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"sum(label)"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"clicks"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;#     print data_wins.columns&lt;/span&gt;
&lt;span class="c1"&gt;# print data_clicks.columns&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_wins&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_clicks&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'algo'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'inner'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withColumn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"effective cpc"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"win_price"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div id="disqus_thread"&gt;&lt;/div&gt;
&lt;script&gt;
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//avoyage.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;
&lt;noscript&gt;Please enable JavaScript to view the &lt;a href="https://disqus.com/?ref_noscript" rel="nofollow"&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</description><category>AdTech</category><category>Data Science</category><category>Machine Learning</category><guid>http://Ayush-iitkgp.github.io/posts/ctr-analysis-using-spark/</guid><pubDate>Wed, 20 Feb 2019 20:43:21 GMT</pubDate></item><item><title>Google Summer of Code 2017</title><link>http://Ayush-iitkgp.github.io/posts/gsoc-2017/</link><dc:creator>Ayush Pandey</dc:creator><description>&lt;h3&gt;&lt;a href="https://summerofcode.withgoogle.com/projects/#5914180975591424"&gt;Proposal&lt;/a&gt;            &lt;a href="https://drive.google.com/open?id=0B2oOdWdSJWa1b3JvRDR1OHZEUTg"&gt;Poster&lt;/a&gt;            &lt;a href="https://github.com/JuliaDiffEq/DiffEqParamEstim.jl/pulls?q=is%3Apr+is%3Aclosed+author%3AAyush-iitkgp"&gt;Github&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Google Summer of Code 2016 under the Julia Language was an experience of a lifetime which motivated me to apply to this year's program as well. While, my last year project with &lt;a href="http://Ayush-iitkgp.github.io/posts/gsoc-2017/"&gt;Convex.jl&lt;/a&gt; was hardcore mathematics, this time I wanted to work in the field of Machine Learning (mainly it's application in the field where there was a potential but not much had been explored). I was well aware of the applications of Machine Learning techniques in Computer Vision, Natural Language Processing, Stock Market Predictions but I never heard or read anyone saying that they used Machine Learning in Differential Equations. So, as soon as I read about the project &lt;a href="https://julialang.org/soc/projects/diffeq.html#machine-learning-tools-for-classification-of-qualitative-traits-of-differential-equation-solutions"&gt;Machine learning tools for classification of qualitative traits of differential equation solutions&lt;/a&gt;, I knew what I had to do in the coming summers. It's been awesome 4 months working on my project and I am so thankful to Google Summer of Code 2017 program and The Julia Language for providing me such an incredible opportunity. The thanksgiving note would be incomplete without the mention of my mentor &lt;a href="http://www.chrisrackauckas.com/"&gt;Chris Rackauckas&lt;/a&gt; who always helped me whenever I felt like giving up.&lt;/p&gt;
&lt;p&gt;In the remaining blog, I will touch upon the technical aspects of my project and my work:&lt;/p&gt;
&lt;h2&gt;Project Abstract&lt;/h2&gt;
&lt;p&gt;Differential equation models are widely used in many scientific fields that include engineering, physics and biomedical sciences. The so-called “forward problem” that is the problem of solving differential equations for given parameter values in the differential equation models has been extensively studied by mathematicians, physicists, and engineers. However, the “inverse problem”, the problem of parameter estimation based on the measurements of output variables, has not been well explored using modern optimization and statistical methods. Parameter estimation aims to find the unknown parameters of the model which give the best fit to a set of experimental data. In this way, parameters which cannot be measured directly will be determined in order to ensure the best fit of the model with the experimental results. This will be done by globally minimizing an objective function which measures the quality of the fit. This inverse problem usually considers a cost function to be optimized (such as maximum likelihood). This problem has applications in systems biology, HIV-AIDS study.&lt;/p&gt;
&lt;p&gt;The expected outcome of my project was &lt;strong&gt;set of a tools for easily classifying parameters using machine learning tooling for users inexperienced with machine learning.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Application - HIV-AIDS Viral Dynamics Study&lt;/h2&gt;
&lt;p&gt;Studies of HIV dynamics in AIDS research are very important for understanding the pathogenesis of HIV infection and for assessing the potency of antiviral therapies. Ordinary differential equation (ODE) models are proposed to describe the interactions between HIV virus and immune cellular response.&lt;/p&gt;
&lt;p&gt;One popular HIV dynamic model can be written as:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;center&gt;&lt;img src="http://Ayush-iitkgp.github.io/images/HIV.png" alt="HIV-AIDS Dynamics Differential Equation" height="300px" width="500px" border="1px" style="margin: 0px 20px"&gt;&lt;/center&gt;
&lt;p&gt;where (T) is target cells which are assumed to be produced at a constant rate s and which are assumed to die at rate d per cell. Productive infection by virus (V), occurs by virus interacting with target cells at a rate proportional to the product of their densities i.e at rate βVT, where β is called the infection rate constant. Productively infected cells (I) are assumed to die at rate δ per cell. Virus is produced from productively infected cells at rate p per cell and is assumed to either infect new cells or be cleared. In the basic model, loss of virus by cell infection is included in the clearance process and virus is assumed to be cleared by all mechanisms at rate c per virion.&lt;/p&gt;
&lt;p&gt;Here we assume that the parameters p and c are known and can be obtained from the literature.&lt;/p&gt;
&lt;p&gt;Very similar to any Machine Lerning problem, we approached the problem of parameter estimation of differential equations by 2 ways: the Bayesian approach (where the idea is to find the probability distribution of the parameters) and the optimization approach (where we are interested to know the point estimates of the parameters).&lt;/p&gt;
&lt;h2&gt;Optimization Approach&lt;/h2&gt;
&lt;p&gt;My mentor Chris had integrated the LossFunctions.jl which builds L2Loss objective function to estimate the parameters of the differential equations. I started by implementing the Two-stage method which is based on based on the local smoothing approach and a pseudo-least squares (PsLS) principle under a framework of measurement error in regression models.&lt;/p&gt;
&lt;p&gt;The following is the comparative study of the above two methods:&lt;/p&gt;
&lt;h3&gt;Advantages&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Computational efficiency&lt;/li&gt;
&lt;li&gt;The initial values of the state variables of the differential equations are not required&lt;/li&gt;
&lt;li&gt;Providing good initial estimates of the unknown parameters for other computationally-intensive methods to further refine the estimates rapidly&lt;/li&gt;
&lt;li&gt;Easing of the convergence problem&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Disadvantages&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;This method does not converge to the global/local minima as the Non-Linear regression does if the cost function is convex/concave.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I also wrote implementations and test cases for supporting different optimizers and algorithms such as:
1. Genetic Algorithm from Evolutionary.jl
2. Stochastic Algorithms from BlackBoxOptim.jl
3. Simulated Annealing, Brent method, Golden Section Search, BFGS algorithm from Optim.jl
4. MathProgBase associated solvers such as IPOPT, NLopt, MOSEK, etc.&lt;/p&gt;
&lt;p&gt;Then, I integrated the PenaltyFunctions.jl with DiffEqParamEstim to add regularization to the loss function such as Tikhonov and Lasso regularization to surmount ill-posedness and ill-conditioning of the parameter estimation problem.&lt;/p&gt;
&lt;h2&gt;Bayesian Approach&lt;/h2&gt;
&lt;p&gt;Our objective was to translate the ODE described in DifferentialEquations.jl using ParameterizedFunctions.jl into the corresponding Stan (a Markov Chain Monte Carlo Bayesian inference engine) code and use Stan.jl to find the probability distribution of the parameters &lt;strong&gt;without writing a single line of Stan code&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The exhaustive list of pull requests can be found &lt;a href="https://github.com/JuliaDiffEq/DiffEqParamEstim.jl/pulls?q=is%3Apr+is%3Aclosed+author%3AAyush-iitkgp"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I am glad to have been successfully implemented what I proposed.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1].&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2631937/"&gt;Parameter Estimation for Differential Equation Models Using a Framework of Measurement Error in Regression Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2].&lt;a href="https://github.com/JuliaDiffEq/DiffEqParamEstim.jl/issues/5"&gt;Parameter estimation: the build_loss_objective #5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3].&lt;a href="http://bmcsystbiol.biomedcentral.com/articles/10.1186/s12918-015-0219-2"&gt;Robust and efficient parameter estimation in dynamic models of biological systems&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4].&lt;a href="http://faculty.bscb.cornell.edu/~hooker/ODE_Estimation.pdf"&gt;Parameter Estimation for Differential Equations: A Generalized Smoothing Approach&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5].&lt;a href="http://www.stat.columbia.edu/~gelman/research/published/stan_jebs_2.pdf"&gt;Stan: A probabilistic programming language for Bayesian inference and optimization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6]. &lt;a href="https://github.com/JuliaDiffEq/DifferentialEquations.jl/issues/135"&gt;Linking with Stan project #135&lt;/a&gt;&lt;/p&gt;
&lt;div id="disqus_thread"&gt;&lt;/div&gt;
&lt;script&gt;
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//avoyage.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;
&lt;noscript&gt;Please enable JavaScript to view the &lt;a href="https://disqus.com/?ref_noscript" rel="nofollow"&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</description><category>GSoC'17</category><guid>http://Ayush-iitkgp.github.io/posts/gsoc-2017/</guid><pubDate>Fri, 25 Aug 2017 20:43:21 GMT</pubDate></item><item><title>One-Class Classification Algorithms</title><link>http://Ayush-iitkgp.github.io/posts/one-class-classification-algorithms/</link><dc:creator>Ayush Pandey</dc:creator><description>&lt;p&gt;In my &lt;a href="http://Ayush-iitkgp.github.io/posts/when-does-traditional-classification-algorithm-fail/"&gt;previous blog entry&lt;/a&gt;, I have put forth a case for when the traditional classification algorithms do not perform well i.e. when the training data has reasonable level of imbalance between the various classes. The &lt;a href="https://drive.google.com/file/d/0B2oOdWdSJWa1NF8ySXhvR1ZvODA/view?usp=sharing"&gt;problem statement&lt;/a&gt; that I was trying to solve had a similar problem of the skewed distribution of the training data. &lt;strong&gt;The power company had only created the database of fraudulent customers&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;As I mentioned in my previous blog post, there are 2 methods to tackle the case when we have data from only one class:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The first method is an obvious approach to generate an artificial second class and proceed with traditional classification algorithms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second one is to modify the existing classification algoriths to learn on the data from only one class. These algorithms are called "one-class classfication algorithms" and include one-class SVM, One-Class K-Means, One-Class K-Nearest Neighbor, and One-Class Gaussian.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this blog entry, I will elaborate on the second method and explain the mathematics behind the one-class classification algorithms and how it improves over the traditional classification algorithms. &lt;/p&gt;
&lt;h3&gt;Fundamental difference between Binary and One Class Classification Algorithms&lt;/h3&gt;
&lt;p&gt;Binary classification algorithms are &lt;em&gt;discriminatory in nature&lt;/em&gt;, since they learn to discriminate between classes using all data classes to create a hyperplane(as seen in the fig. below) and use the hyperplane to label a new sample. In case of imbalance between the classes, the discriminatory methods can not be used to their full potential, since by their very nature, they rely on data from all classes to build the hyperplane that separate the various classes.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;center&gt;&lt;img src="http://Ayush-iitkgp.github.io/images/binaryClassification.png" alt="Binary Classification Hyperplane" height="200px" width="375px" border="1px" style="margin: 0px 20px"&gt;Binary Classification Hyperplane&lt;/center&gt;&lt;br&gt;
&lt;p&gt;Where as the one-class algorithms are based on &lt;em&gt;recognition&lt;/em&gt; since their aim is to recognize data from a particular class, and reject data from all other classes. This is accomplished by creating a boundary that encompasses all the data belonging to the target class within itself, so when a new sample arrives the algorithm only has to check whether it lies within the boundary or outside and accordingly classify the sample as belonging to the target class or the outlier.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;center&gt;&lt;img src="http://Ayush-iitkgp.github.io/images/oneClassClassification.png" alt="One-Class Classification Boundary" height="200px" width="375px" border="1px" style="margin: 0px 20px"&gt;One-Class Classification Boundary&lt;/center&gt;&lt;br&gt;
&lt;h3&gt;Mathematics behind different One-Class Classification Algorithms&lt;/h3&gt;
&lt;p&gt;In this section, I will explain the mathematics behind different one-class machine learning algorithms by taking a cue from &lt;a href="http://file.scirp.org/pdf/JBiSE20100300003_45072138.pdf"&gt;this research paper&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;One-Class Gaussian&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;This is basically a density estimation model. It assumes that the training data are the samples from the &lt;em&gt;Multivariate Normal Population&lt;/em&gt;, therefore for a test sample (say z) having n-feaures, the probability of it belonging to the target class can be calculated as follows:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;center&gt;&lt;img src="http://Ayush-iitkgp.github.io/images/one-classGaussian.png" alt="one-class gaussian" height="100px" width="375px" border="1px" style="margin: 0px 20px"&gt;&lt;/center&gt;&lt;br&gt;
&lt;p&gt;where the parameters μ and Σ are the &lt;em&gt;poputation mean and covariance&lt;/em&gt;. Hence, the objective function of this machine learning algorithm is to determine the estimates for μ and Σ. Using the &lt;strong&gt;method of maximum likelihood estimator&lt;/strong&gt;, we can show that the sample mean and sample covariance are the unbiased and consistent estimators for population mean and variance respectively. Hence, once we calculate the probability p(z), we can set a threshold to determine whether a new sample is outlier or not.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;One-Class Kmeans&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;In this method, we first classify the training data into k clusters (which is chosen as per our requirements). Then, for a new sample (say z), the distance &lt;em&gt;d(z)&lt;/em&gt; is calculated as the minimum distance of the sample from the centroid of all the k clusters. Now if &lt;em&gt;d(z)&lt;/em&gt; is less than a particular thereshold (which is again chosen as per our requirements), then the sample belongs to the target class otherwise it is classified as the outlier.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;One-Class K-Nearest Neighbor&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Let us take note of some notation before we understand the mathematics behind the algorithm.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;d(z,y)&lt;/em&gt; : distance between two samples z and y&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NN(y)&lt;/em&gt; : Nearest Neighbor of sample y&lt;/p&gt;
&lt;p&gt;Now given a test sample z, we find the nearest neighbor of z from the training data (which is NN(z) = y) and the nearest neighbor of y (which is NN(y)). Now the rule is to classify z as belonging to the target class when:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;center&gt;&lt;img src="http://Ayush-iitkgp.github.io/images/K-NearestNeighbor.png" alt="K-Nearest Neighbor" height="100px" width="375px" border="1px" style="margin: 0px 20px"&gt;&lt;/center&gt;&lt;br&gt;
&lt;p&gt;where the default value of δ is 1 but can be chosen to satisfy our requirements.&lt;/p&gt;
&lt;p&gt;In my next blog entry, I will try to explain the most widely used one-class classification algorithm i.e. one-class support vector machines. Stay tuned !&lt;/p&gt;
&lt;p&gt;Thank you very much for making it this far.&lt;/p&gt;
&lt;div id="disqus_thread"&gt;&lt;/div&gt;
&lt;script&gt;
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//avoyage.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;
&lt;noscript&gt;Please enable JavaScript to view the &lt;a href="https://disqus.com/?ref_noscript" rel="nofollow"&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</description><category>Application of Novelty Detection Algorithms to predict Electricity Theft</category><category>Data Science</category><category>Machine Learning</category><guid>http://Ayush-iitkgp.github.io/posts/one-class-classification-algorithms/</guid><pubDate>Sun, 22 Jan 2017 06:43:21 GMT</pubDate></item><item><title>When does traditional classification algorithm fail?</title><link>http://Ayush-iitkgp.github.io/posts/when-does-traditional-classification-algorithm-fail/</link><dc:creator>Ayush Pandey</dc:creator><description>&lt;p&gt;One of the most intriguing Machine Learning problems that I have come across was during my 3rd year in the college where a startup named Quantta Analytics presented us with the problem statement about a power distribution company which had observed a significant loss of revenue over a period of time. The loss in revenue was mainly becuase of possible power theft by malicious consumers. The power company resorted to periodic vigilance of customers by sampling and &lt;strong&gt;creating a database of only the fraudulent customers&lt;/strong&gt; to curb this practice. The company wanted the vigilance process to be more robust and effective. Hence, the objective of the problem statement was to deploy a machine learning algorithm to provide a list of customers who are likely to commit fraud. So the problem was effectively a classification problem (&lt;strong&gt;with a catch!&lt;/strong&gt;) where given the attributes of a customer, we had to predict where it is likely to commit the electricity theft or not.&lt;/p&gt;
&lt;p&gt;In order to appreciate the problem statement, let us first have the review of the well-known famous classification algorithms. Classification problems try to solve the two or multi-class situation. The goal is to distinguish test data between a number of classes, using training data which has samples from all the possible classes and &lt;strong&gt;training data has reasonable level of balance between the various classes&lt;/strong&gt;. Now the question that arise is - &lt;em&gt;At what levels of imbalance does the use of traditional classifiers becomes futile?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://ieeexplore.ieee.org/document/6406735/"&gt;This paper&lt;/a&gt; has made observation by conducting experiments on various datasets from the UCI repository, and monitoring the performance of the traditional classifiers. I am listing down the most important observations stated in the paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The performance of the traditional classifiers start to decline when the imbalance between output classes increase and &lt;strong&gt;the decline becomes prominent at the ratio 1:2.8&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;At the ratio 1:10&lt;/strong&gt;, the performance is so poor for traditional classifiers that it can no longer be trusted.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Figure below shows the initial ratio of the classes present in UCI dataset and the ratio at which the performance of the binary classiﬁers starts to deteriorate.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;center&gt;&lt;img src="http://Ayush-iitkgp.github.io/images/BinaryPerformanceTable.png" alt="Binary Classification Algorithm Performance Table" height="200px" width="375px" border="1px" style="margin: 0px 20px"&gt;&lt;/center&gt;
&lt;p&gt;Now the questions that arise are - what if we have the training data which has imbalance between the classes or data only from one class? Why do we need to study such case? And are there any situations where such data is available?&lt;/p&gt;
&lt;p&gt;To answer the latter first, yes there are plenty of situations where we have the data from only one class. Consider a case of a nuclear power plant. We have the measurement of the plant conditions such as temperature, reaction rate when the plant is in working condition. Is it possible to get such measurements in case of an accident? No. Now, if we want to predict the possible scenario of the breakdown of the plant. From the above observations it is sure that the traditional classification algorithms will not perform well. Some other cases are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Detection of oil spill&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In computational biology to predict microRNA gene target&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are 2 methods to tackle the case when we have data from only one class:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The first method is an obvious approach to generate an artificial second class and proceed with traditional classification algorithms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second one is to modify the existing classification algoriths to learn on the data from only one class. These algorithms are called "one-class classfication algorithms" and include one-class SVM, One-Class K-Means, One-Class K-Nearest Neighbor, and One-Class Gaussian.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I will be elaborating on the above two methods in my next blog post. Thank you very much for making it this far.&lt;/p&gt;
&lt;div id="disqus_thread"&gt;&lt;/div&gt;
&lt;script&gt;
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//avoyage.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;
&lt;noscript&gt;Please enable JavaScript to view the &lt;a href="https://disqus.com/?ref_noscript" rel="nofollow"&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</description><category>Application of Novelty Detection Algorithms to predict Electricity Theft</category><category>Data Science</category><category>Machine Learning</category><guid>http://Ayush-iitkgp.github.io/posts/when-does-traditional-classification-algorithm-fail/</guid><pubDate>Tue, 17 Jan 2017 06:43:21 GMT</pubDate></item><item><title>Is it the end or another beginning ?</title><link>http://Ayush-iitkgp.github.io/posts/is-it-the-end-or-another-beginning/</link><dc:creator>Ayush Pandey</dc:creator><description>&lt;h3&gt;&lt;a href="http://nbviewer.jupyter.org/github/Ayush-iitkgp/GSoc-Proposal/blob/master/GSoC%202016%20Application%20Ayush%20Pandey-%20Support%20for%20complex%20numbers%20within%20Convex.jl.ipynb"&gt;Proposal&lt;/a&gt;            &lt;a href="https://ayush-iitkgp.github.io/categories/gsoc16/"&gt;Blog&lt;/a&gt;            &lt;a href="http://Ayush-iitkgp.github.io/stories/juliacon-2016-talk/"&gt;Talk&lt;/a&gt;            &lt;a href="https://drive.google.com/file/d/0B2oOdWdSJWa1cUFKTGI0czFDSDQ/view?usp=sharing"&gt;Presentation&lt;/a&gt;            &lt;a href="https://github.com/Ayush-iitkgp/Convex.jl/tree/gsoc2"&gt;Github&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;It hasn't been long when I was hugging my friends, calling my parents to inform them of my selection to Google Summer of Code, 2016 program and now here I am writing a wrap-up post for the program. I remember wanting to spend my summers working on the project that involved lots of mathematics, some of the computer science and travel as part of the project. I am so thankful to Google Summer of Code, 2016 program and The Julia Language to have made my each and every wish come true. The thanksgiving note would be incomplete without the mention of my mentors Madeleine and Dj who always helped me whenever I felt like giving up.&lt;/p&gt;
&lt;p&gt;Now, coming to the technical aspects of my project, it was divide in 3 phases based on the branches of convex programming namely:&lt;/p&gt;
&lt;h4&gt;1. Support for complex-domain linear programs&lt;/h4&gt;
&lt;p&gt;During this phase of the project, I extended the present implementation in Convex.jl to provide support for linear programs involving complex variables and the complex coefficients. The technical details of the implementation are described in the blog post &lt;a href="https://ayush-iitkgp.github.io/posts/announcing-support-for-complex-domain-linear-programs-in-convexjl/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;2. Support for second order conic programs&lt;/h4&gt;
&lt;p&gt;In this phase of the project, I in consulation with my mentors, rewrote many the second order cone atoms such as abs, norm to accept complex arguments. We also had an intense discussion on whether redefining the above atoms to accept complex arguments would violate DCP compliance and we came to a conclusion that defining inverse or the square atoms on complex variables neither makes sense nor does it preserve the DCP compliance.&lt;/p&gt;
&lt;h4&gt;3. Support for Complex Semidefinite programs&lt;/h4&gt;
&lt;p&gt;The above 2 phases were relatively difficult for us as we had no literature references and the decision we made were solely based on our understanding and intuition. During this phase, we used the mapping in the introductory section of the research paper &lt;a href="http://arxiv.org/pdf/1007.2905v2.pdf"&gt;here&lt;/a&gt; to transform a complex semidefinite program to the corresponding real semidefinite program. Presently, I am writing test cases to check the correctness of our implementation.&lt;/p&gt;
&lt;p&gt;The exhaustive list of commits can be found &lt;a href="https://github.com/Ayush-iitkgp/Convex.jl/commits/gsoc2"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I am glad to have been successfully implemented what I proposed. Presently, I am also writing the documentation and examples to demonstrate the usability of my implementation. The project will culminate with a single pull request to the Convex.jl repository as well as the release of a new version of Convex.jl which we plan to do in next few days. &lt;/p&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;[1].&lt;a href="http://www.sciencedirect.com/science/article/pii/S0022000003001454"&gt;Approximation algorithms for MAX-3-CUT and other problems via complex semidefinite programming&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2].&lt;a href="http://arxiv.org/pdf/1007.2905v2.pdf"&gt;Invariant semidefinite programs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3].&lt;a href="http://arxiv.org/pdf/1410.4821.pdf"&gt;Convex Optimization in Julia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4].&lt;a href="https://github.com/JuliaOpt/Convex.jl/issues/103"&gt;Support for complex variables&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5].&lt;a href="https://github.com/cvxgrp/cvxpy/issues/191"&gt;Add complex variables&lt;/a&gt;&lt;/p&gt;
&lt;div id="disqus_thread"&gt;&lt;/div&gt;
&lt;script&gt;
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//avoyage.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;
&lt;noscript&gt;Please enable JavaScript to view the &lt;a href="https://disqus.com/?ref_noscript" rel="nofollow"&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</description><category>GSoC'16</category><guid>http://Ayush-iitkgp.github.io/posts/is-it-the-end-or-another-beginning/</guid><pubDate>Fri, 19 Aug 2016 20:43:21 GMT</pubDate></item><item><title>Announcing support for complex-domain linear Programs in Convex.jl</title><link>http://Ayush-iitkgp.github.io/posts/announcing-support-for-complex-domain-linear-programs-in-convexjl/</link><dc:creator>Ayush Pandey</dc:creator><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In recent years, programmatic advertising is been taking over the online advertisement industry. Many companies referred to as DSP (Demand Side Platform) compete for the same ad-slot on the internet. The success of the DSPs to deliver the values to the advertisers is evaluated on the below two criteria:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;High Click Through Rate = Number of clicks/ Number of ads shown&lt;/li&gt;
&lt;li&gt;High Conversion Rate = Number of conversions (such as a purchase)/ Number of ads shown&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To achieve high CTR and Conversion Rates, DSPs heavily rely on using Artificial Intelligence techniques and develop their in-house Machine Learning algorithms. The problem of applying Machine Learning in Adtech is different from the standard problem in many sense. Google's paper &lt;a href="https://ai.google/research/pubs/pub41159"&gt;&lt;em&gt;Ad Click Prediction: a view from the Trenches&lt;/em&gt;&lt;/a&gt; and Facebook's paper &lt;a href="https://research.fb.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/"&gt;&lt;em&gt;Practical Lessons from Predicting Clicks on Ads at Facebbok&lt;/em&gt;&lt;/a&gt; have discussed in details the lessons learned while building an AI for the adtech industry. In the remaining blog post, I will try to summarize the intricacies of applying ML in adtech and how it is tackled in general:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Large size of the training vector:&lt;/strong&gt; Every feature in the ML model is a categorical feature and encoding them into numerical feature explodes the size of the training vector to the order of billions. For example, one of the most important features in the ML model is the &lt;em&gt;publisher&lt;/em&gt; website where the ad would be displayed which is a categorical feature and there are millions of publishers so using one-hot encoding would result in a training vector of million entries.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Skewness of the training data:&lt;/strong&gt; Typically, CTRs are much lower than 50% (generally, CTR is around 1-2%)which means that positive examples (clicks) are relatively rare, hence is the problem of skewness in the training data is introduced.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rapid changes in the online ad landscape:&lt;/strong&gt; The adtech domain is a very dynamic environment where the data distribution changes over time. Facebook conducted an experiment where they trained the model on one day of data and evaluated on the six consecutive days. The results showed that the performance of the model decreases as the delay between the training and test set increases. Thus, it is very important to update the model every few hours to keep it very real-time&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Per-coordinate learning rate:&lt;/strong&gt; In most of the standard ML problems, the learning rate is a constant. In adtech, there is a huge imbalance of the number of training instances on each feature. For example, a famous publisher such as cnn.com will be having more users thus more ad-slots compared to a very unknown one thus, our training data will have a huge number of training instances for cnn.com. Therefore, we want to decrease the learning rate for the coordinate as its frequency in the training data increases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Using Progressive Validation rather than cross-validation:&lt;/strong&gt; Validating the model on the data set which lags the train set by hours or days is not a good idea as we discussed above that the nature of the dataset changes with time. The &lt;em&gt;online log loss&lt;/em&gt; is instead a good proxy for the model performance because it measures the performance only on the most recent data before we train on it, this is exactly analogous to what happens when the model is in production. This also ensures that we can use 100% of our data for both training and testing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relative changes in the metric compared to Absolute metric:&lt;/strong&gt; Click rates vary from country to country and from ad-slot to ad-slot and therefore the metrics change over the course of a single day. Google experiments indicate that the relative changes(compared to a baseline model) are much more stable over time, therefore, a relative change in logloss is a better metric than the average log loss. We also take care only to compare metrics computed from exactly the same data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Segmented performance metrics instead of aggregated metrics:&lt;/strong&gt; One of the things that we have to take care while analyzing the performance of the models in adtech is that the aggregated performance metrics may hide effects that are specific to certain sub-populations of the data. For example, a high CTR may, in fact, be caused by a mix of low and high CTR from different ad exchanges. This makes it critical to visualize the performance metrics not only on the aggregate data but also on the various slicing of the data such as per ad exchange, per adgroup, per device type, per publisher.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This blog is the aggregation of all the lessons learnt while working as Data Scientist in the AdTech company. Do let me know if you have any additional comments. &lt;/p&gt;
&lt;p&gt;Do you have any questions?&lt;/p&gt;
&lt;p&gt;Ask your questions in the comments below and I will do my best to answer.&lt;/p&gt;
&lt;div id="disqus_thread"&gt;&lt;/div&gt;
&lt;script&gt;
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//avoyage.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;
&lt;noscript&gt;Please enable JavaScript to view the &lt;a href="https://disqus.com/?ref_noscript" rel="nofollow"&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;</description><category>GSoC'16</category><guid>http://Ayush-iitkgp.github.io/posts/announcing-support-for-complex-domain-linear-programs-in-convexjl/</guid><pubDate>Tue, 26 Jul 2016 20:43:21 GMT</pubDate></item></channel></rss>