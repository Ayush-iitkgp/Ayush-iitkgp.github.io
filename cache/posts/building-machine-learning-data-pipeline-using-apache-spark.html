<h1>Introduction</h1>
<p><a href="">Apache Spark</a> is increasingly becoming popular in the field of Data Sciences because of its ability to deal with the huge datasets and the capability to  run computations in memory which is particularly useful in iterative tasks such as the training step of the Machine Learning algorithm. As part of the Data Engine team at <a href="https://www.sprinklr.com/">Sprinklr</a>, I had some experience building the data processing pipeline in Spark. In this blog post, I will try to summarize my learning in simpler, easy to understand terms along with the python code.  </p>
<p><strong>Q. Why is Apache Spark a suitable tool for building the ML data pipeline?</strong></p>
<p><strong>Ans.</strong> Few years ago, <a href="https://scikit-learn.org/stable/">scikit-learn</a> came up with the idea of data pipeline but with the advent of big data, it became very problematic to scale. Spark's data pipeline concept is mostly inspired by the scikit-learn project. It provides the APIs for machine learning algorithms which make it easier to combine multiple algorithms into a single pipeline, or workflow.</p>
<p>Now, I will introduce the key concepts used in the Pipeline API:</p>
<p><strong>DataFrame:</strong> It is basically a data structure for storing the data in-memory in a highly efficient way. Dataframe in Spark is conceptually equivalent to a dataframe in R/Python. It can store  different data types such a string, vectors, true labels, and predictions. Dataframes can be created from the csv, json and many different file formats stored on the local filesystem, Hadoop HDFS or cloud environment such as AWS S3.</p>
<p><strong>Transformer:</strong> It is a method or an algorithm which can transform one DataFrame into another DataFrame. It includes SQL statements, feature transformers and learned ML models. While defining a transformer, you have to specify the column it would operate on and the output column it would append to the input DataFrame. Technically, a Transformer implements a method <strong>transform()</strong>. E.g., a SQL <em>select</em> statement which would return a new dataframe with only required columns. Another example is a <em>trained ML model</em> which turns a dataframe with feature vectors into a dataframe with predictions.</p>
<p><strong>Estimator:</strong> It is an algorithm which can be fit on a DataFrame to produce a Transformer.  Technically, an Estimator implements a method <strong>fit()</strong> which accepts a DataFrame and produces a <strong>Model</strong> which is a Transformer. For example, a machine learning algorithm is an Estimator which trains on a DataFrame and produces a trained model which is a transformer as it can transform a feature vector into predictions.</p>
<p><strong>Parameter:</strong> These are the hyperparameters used during cross-validation phase of the ML pipeline.</p>
<p><strong>Pipeline:</strong> A Pipeline is a sequence of PipelineStage (Transformers and Estimators)together to be running in a particular order to specify a Machine Learning workflow. <strong>A Pipeline’s stages are specified as an ordered array</strong>. For example predicting the price of a house given it's breadth, length, location and age involve several stages:</p>
<ul>
<li>Remove the data points which have all columns as null value - Transformer</li>
<li>Create a new feature <strong>area</strong> - Transformer</li>
<li>Learn a prediction model using the feature vectors and the actual price - Estimator</li>
<li>Use the learned model to predict the prices - Transformer</li>
</ul>
<p>A Pipeline is an <em>Estimator</em> in itself. Thus, after a Pipeline’s fit() method is called, it produces a <em>PipelineModel</em>, which is a Transformer. </p>
<p>PipelineModel has the same number of stages as the original Pipeline, <strong>but all Estimators in the original Pipeline have become Transformers</strong>. This PipelineModel is used at test time. </p>
<p>Let's dive into the Python code using an example mentioned in the Spark's doc <a href="https://spark.apache.org/docs/latest/ml-pipeline.html#example-pipeline">here</a> where we are trying to classify a line of text into 0 or 1:</p>
<p><strong>Step 1: Create a DataFrame</strong></p>
<div class="code"><pre class="code literal-block"><span class="c1"># Some import statements</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.classification</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.feature</span> <span class="kn">import</span> <span class="n">HashingTF</span><span class="p">,</span> <span class="n">Tokenizer</span>

<span class="c1"># DataFrame could be created from a csv file or any other sources </span>
<span class="n">training</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span>
<span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;a b c d e spark&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
<span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;b d&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
<span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;spark f g h&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
<span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;hadoop mapreduce&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
<span class="p">],</span> <span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">])</span>

<span class="c1"># Let&#39;s also create a test dataset which we will use later</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span>
<span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;spark i j k&quot;</span><span class="p">),</span>
<span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;l m n&quot;</span><span class="p">),</span>
<span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="s2">&quot;spark hadoop spark&quot;</span><span class="p">),</span>
<span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="s2">&quot;apache hadoop&quot;</span><span class="p">)</span>
<span class="p">],</span> <span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">])</span>
</pre></div>

<p><strong>Step 2: Specify the transformers and the estimators of the pipeline</strong></p>
<div class="code"><pre class="code literal-block"><span class="p">#</span><span class="w"> </span><span class="n">Tokenizer</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">transformer</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">convert</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">text</span><span class="w"> </span><span class="n">column</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">words</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">space</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">delimeter</span>
<span class="n">tokenizer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Tokenizer</span><span class="p">(</span><span class="n">inputCol</span><span class="o">=</span><span class="s">&quot;text&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">outputCol</span><span class="o">=</span><span class="s">&quot;words&quot;</span><span class="p">)</span>

<span class="p">#</span><span class="w"> </span><span class="n">HashingTF</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">again</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">transformer</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">takes</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">column</span><span class="w"> </span><span class="s">&quot;words as an input and creates a new column of a vector&quot;</span><span class="w"> </span>
<span class="n">hashingTF</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">HashingTF</span><span class="p">(</span><span class="n">inputCol</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">getOutputCol</span><span class="p">(),</span><span class="w"> </span><span class="n">outputCol</span><span class="o">=</span><span class="s">&quot;features&quot;</span><span class="p">)</span>

<span class="p">#</span><span class="w"> </span><span class="n">Logistic</span><span class="w"> </span><span class="n">Regression</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">Estimator</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="s">&quot;features&quot;</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="s">&quot;label&quot;</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">input</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">create</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">trained</span><span class="w"> </span><span class="n">model</span>
<span class="n">lr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">maxIter</span><span class="o">=</span><span class="mh">10</span><span class="p">,</span><span class="w"> </span><span class="n">regParam</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>

<p><strong>Step 3: Create the pipeline using the transformers and the estimators defined in step 2</strong></p>
<div class="code"><pre class="code literal-block">pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])
</pre></div>

<p><strong>Step 4: Call the fit() method on the pipeline to create a PipelineModel</strong></p>
<div class="code"><pre class="code literal-block">model = pipeline.fit(training)
</pre></div>

<p><strong>Step 5: Use the PipelineModel to do the predictions of the test dataset</strong></p>
<div class="code"><pre class="code literal-block"><span class="nv">prediction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">model</span>.<span class="nv">transform</span><span class="ss">(</span><span class="nv">test</span><span class="ss">)</span>
<span class="nv">selected</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">prediction</span>.<span class="nv">select</span><span class="ss">(</span><span class="s2">&quot;id&quot;</span>,<span class="w"> </span><span class="s2">&quot;text&quot;</span>,<span class="w"> </span><span class="s2">&quot;probability&quot;</span>,<span class="w"> </span><span class="s2">&quot;prediction&quot;</span><span class="ss">)</span>
<span class="nv">selected</span>.<span class="k">show</span><span class="ss">()</span>
</pre></div>

<p>One of the big benefits of the Machine Learning Data Pipeline in Spark is hyperparameter optimization which I would try to explain in the next blog post. I hope this blog would help you in getting started with Spark for building ML data pipelines. </p>
<div id="disqus_thread"></div>
<script>
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//avoyage.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>